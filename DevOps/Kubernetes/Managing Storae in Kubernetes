Managing Storae in Kubernetes 
=====================================

Persistent Storage and Containers 
-----------------------------------

1) Containers are ephemeral
2) each time a container is destroyed the container's writable layer is deleted.
3) When a POd is delete, its container(s) is deleted from the node 
4) Now the qiestion is how we can persist data across a Pod's lifecycle ? kubernetes gives us some API objects to help with this question.


Storage API Objects in Kubernetes
----------------------------------------------

1) Volume    - This the persistent volume that gonna be the poart of Pod spec 
2) persistent Volume - this is the actaul volume itself inside availbale in the cluster and is used by Pods 
3) Persistent Volume claim - this is a request for persistent volume made by a user 
4) Storage Class - way for us to offer groups and classes of storgae availbale in the cluster and will be used bu users and thier pod base applications


Volume 
--------------

1) Persistent Storage deployed as a prt of the Pod spec 
2) Implementation details for yur storage *( we have to proide all the details like server name, Ip etc)
3) This can be little challenging 
   a) If we take this pod definition to other environment, we have to rework the pod sepc for that environment (this limits portability and sharing code)
   b) Additionaly Volumes has same lifecycle as Pods when a pod is destroyed so is its volume definition.
4) we can do better ....


persistent Volume
-----------------------

1) Adminstrator defined storage in the cluster 
2) Implementation details for your storage 
3) Lifecycle of persistent Volume is completly independent of our Pod
4) the objects will be created by the API server but the actual things will be managed by kubelet 
    a) Like Maping the Storage in the node 
	b) Exposes persistent Volume as a mount inside the container 
5) Types of persistent Volume (Grouped)
----------------------------------------

1) Networked - we will have things like NFS, AzureFile
2) Blocked - we will have Fibre Channel and iSCSI
3) Cloud - we will have AWS EBS, Azure Disk , GCE persistent Disk 


Persistent Volume claim
-------------------------------

1) A request for storage by a user 
2) we need to specify few thinsgs like - size, Access Mode, Storage class 
3) Persistent Volume claim enables portability of your application configurations 
4) The cluster will map a Persistent Volume claim to a persistent Volume


Controlling persistent Volumes
-----------------------------------

We need to define access more which will control how a node or potential nodes will access Persistent Volume. as Persistent Volume are mapped to Nodes by kubelet 
and exposed into the pods 

3 types of Access Mode
-----------------------

1)  ReadWriteOnce (RWO) - One Node can mount a volume for read and write access 
2)  ReadWriteMany (RWX) - More than one node can mount a volume for read and write access
3)  ReadOnlyMany (ROX) - More than one node can mount a volume for read Only access

Access Mode is a core element to map the Persistent Volume claim back to a Persistent Volume and this mapping will be based of Size, access Mode and storage class


Static Provisioning Workflow 
----------------------------------------

1) create a Persistent Volume 
2) Create a Persistent Volume claim
3) define Volume in a Pod spec 

Doing all three above manually is called Static Provisioning

Storage Lifecycle
---------------------

1) Binding - It is a process of mapping a Persistent Volume claim to Persistent Volume for access by a Pod (either static or dynamic provisioning)
2) Using - when a PVC is being used by a Pod for it's lifetime
3) Reclaim - when a PVC is deledted and PC can be claimed as per the PersistentVolumeReclaimPolicy (below 2 options)
  a) Delete (default) - it tells the underlying storage provider that it's OK to delete the actula storage  
  b) Retain - we want to hold on that data for  little longer and will Adminstratorly deleted 


Defining a persistent Volume
--------------------------------------

type { nfs, fc, azuredisk, ... }
capacity
accessModes
persistentVolumeReclaimPolicy
Labels 


apiVersion: apps/v1
kind: PersistentVolume
metadata:
	name: pv-nfs-data
spec:
  capacity: 
	storage: 10Gi 
  accessModes: 
	- ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
nfs:	
	server: 172.126.167.5
	path: "/export/volumes/pod"


Defining a persistent Volume Claim
--------------------------------------

accessModes
resources
storageClassName
selector

apiVersion: apps/v1
kind: PersistentVolume
metadata:
	name: pvc-nfs-data
spec:
  accessModes: 
	- ReadWriteMany
  resources:
	requests:
	  storage: 10Gi


Understanding Persistemt Volume in Pods 
-----------------------------------------

we have to make chnages for storage realted in Pods spec 

....
spec:
 voulmes:
	- name: webcontent
	  persistentVolumeClaim:
	     claimName: pvc-nfs-data
 containers:
	- name: nginx
	...
      volumeMounts:
	  - name: webcontent
	    mountpath: "/usr/share/nginx/htm/web-app/"


DEMO: Storage server Overview 
-------------------------------------
# bulid a new storage server  
# Install NFS Server and create the directory for our exports 

** sudo spt install nfs-kernel-server
** sudo mkdir /export/volumes
** mkdir /export/volumes/pod

# configure our NFS Export in /etc/export for /export/volumes. Using no_root_squash and no _subtree_check to
# allow applications to mount subdirectories of the export directly 

** sudo bash -c ' echo "/export/volumes *(rw,no_root_squash,no _subtree_check)" >/etc/exports'
** cat /etc/exports
** sudo systemctl restartnfs-kernel-server.service 
** exit 

# On each Node in our cluster... install NFS clinet 

** sudo apt install nfs-common 

# On of the Nodes, test out the basic NFS access before moving on.

** ssh user@<node_IP>
** sudo  mount -t nfs4 <storage_server>:/export/volumes /mnt/
** mount | grep nfs
** sudo unmount /mnt/
** exit 


DEMO: Static provisioning persistent Volumes
---------------------------------------------------

# Create a persistent volume with read/write many and retain as the claim Policy 

** kubectl apply -f nfs.pv.yaml 

# review the status, created sources, Access mode and reclism policy is set to Reclaim rather than delete.

** kubectl get PersistentVolume pv-nfs-data

# look at more closly at the PV and it's configurations

** kubectl describe PersistentVolume pv-nfs-data

# Create a PVS on that PV 

** kubectl apply -f nfs.pvs.yaml 

# Check the status, now it's Bound due to the PVC on the PV. see the claim 

** kubectl get PersistentVolume


DEMO: Using a persistent Volume in Pod 
-----------------------------------------------

# We need to write some data in the storage server so that we can use it later

** ssh <storage_server>
** sudo bash -c 'echon "Hello from our NFS mount!!! > /export/volumes/pod/demo.html'
** more /export/volumes/pod/demo.html
** exit 

# Lets' create a Pod (in deployment and add a Service) with PVC-nfs-data

** kubetcl apply -f nfs.nginx.yaml
** kubetcl get service nginx-nfs-service
** SERVICEIP=$ (kubectl get service | grep nginx-nfs-service | awk '{ print $3}'

# Check to see if our pods are running before proceeding 

** kubectl get pods 

# let's access that application to see our application data...

** curl http://$SERVICEIP/web-app/demo.html

# Check the mounted by output for which Pod(s) are accessing this storage 

** kubectl describe PersistentVolumeClaim pv-nfs-data

# if we go inside the pod/container, let's look at where the PV is mounted 

**  kubectl exec -it <pod_name>  --/bin/bash
** ls /usr/share/nginx/htm/web-app/
** more /usr/share/nginx/htm/web-app/demo.html
** exit 

# what node is this pod on ?

** kubectl get pods -o wide 

# lets log into the node and look at the mounted volumes .. it's kubelets job to make the device/mount available.

** ssh <node_name>
** mount | grep nfs
** exit 

# let's delete the pod and see if we still have access to our data in our PV..

** kubectl get pods 
** kubectl delete pod <pod_name> 

# we get a new pod ...ut is our app still there 

** kubectl get pods 

# let's access that application again to see our application data...YES!

** curl http://$SERVICEIP/web-app/demo.html


DEMO: Controlling Persistent Volume Access with Access Modes and reclaim Plocy  
------------------------------------------------------------------------------------

# Controlling PV access with AccessMode and persistentVolumeReclaimPolicy
# Scale the deploymnet to 4 replicas

** kubectl scale deployment nginx-nfs-deployment --replicas=4

# Now look at who's attached to PVC, all $ pods 
# Our Access Mode for this PV and PVC is RWX ReadWriteMany

** kubectl describe persistentVolumeClaim  

# we still can access that application and data... (loadbalanced among 4 pods)

** curl http://$SERVICEIP/web-app/demo.html

# let's delete our deployment

** kubectl delete deployment <deployment_name>

# Check status, still bound on PV... why is that 

** kubectl get PersistentVolume

# becuase the PVC is stil exists 

** kubectl get persistentVolumeClaim

# Can re-use the same PVS and PV from a Pod definition..yes! Because I didn't delete the PVC.

** kubectl apply -f nfs.nginx.yaml

# Our app is up and running

** kubetl get pods 

# but if I dlete the deployment as well as persistentVolumeClaim 

** kubectl delete deployment nginx-nfs-deployment
** kubectl delete persistentVolumeClaim pvc-nfs-data

# Check the status it's released now

** kubectl get PersistentVolume

# Lets try to use the PersistentVolume and see what happened, recreate the PVC for this PV

** kubectl apply -f nfs.nginx.yaml

# Then try to use the PVC/PV in a POd definition

** kubectl apply -f nfs.nginx.yaml 

# letc check the Pod status again 
** kubectl get pods 

# As PVC status is....Pending...becasue PV is released and our reclaim Policy is retain 

** kubectl get PersistentVolumeClaim
** kubectl get PersistentVolume

# for now Let's delete the PV if we want to resue the exact PV ... to 're-create' the PV

** kubectl delete deployment <deployment_name>
** kubectl delete persistentVolumeClaim <PVC_name>
** kubectl delete persistentVolume <PV_name>

# if we recrete the PV, PVC and the pods we will be able to re-deployed
# the cleanup of the dtata is defined by the reclaim policy. (it will be useful in dynamic provisioning)
# But in this case, since it's NFS , we have to clean it up and remove the file.
# Nothing will prevent a user from getting this access to this data, so it's imperitive to clean up 

** kubectl apply -f nfs.pv.yaml
** kubectl apply -f nfs.pvc.yaml
** kubectl apply -f nfs.nginx.yaml
** kubectl get pods

***********************************************************************************************************************************************************************

Storage Class (Core of dynamic provisioning)
-------------------------------------------------

1) Define Tiers/classes of storage and attached attributes
2) Enables dynamic Provisioning 
3) Define infrastructure specific parameters
4) Reclaim Policy 



Dynamic provisioning Workflow
----------------------------------------

1) Create a StorageClass
2) Create a PersistentVolumeClaim
3) Define Volume in Pod Spec
4) Creates a PersistentVolume

Defining a Storgae class in micrSoft Azure
----------------------------------------------

apiVersion: storage.k8s.io/v1
kind: StoragClass
metadata:
	name: managed-premium
	parameters: 
		kind: Managed
		storageaccounttype: Premium_LRS
		provisioner: kubernetes.io/azure-disk


Defining a persistent Volume Claim in micrSoft Azure
------------------------------------------------------

apiVersion: apps/v1
kind: PersistentVolume
metadata:
	name: pvc-azure-managed
spec:
  accessModes: 
	- ReadWriteOnce
  storageClassName: managed-premium
  resources:
	requests:
	  storage: 10Gi


DEMO: Static provisioning persistent Volumes in MS Azure 
-------------------------------------------------------------

# switch to Azure cluster context 

** kubectl config use-context 'k8s-cloud'
**kubectl get nodes 

# let's create a disk in Azure. Using a dynamic provisioner and storgae class
# Checkout ourl list of available storage class, which one is default? Notice the provisioner, parameters and Recalim Policy.

** kubectl get StorageClass
** Kubectl descrie StorageClass default
** kubectl describe StorageClass managed-premium

# lets create a Deploymnet of an nginx pod with ReadWriteOnce disk
# we create a PVC and a deployment that creats a pod that use that PVC 

** kubectl apply -f AzureDisk.yaml


# checkout the accessmode, Reclaim Policy, Status, Claim and Storage Class

** kubectl get PersistentVolume
** kubectl get PersistentVolumeClaim
 
# Lets see if our Pod is created 

** kubectl get pods


DEMO: Defining a custome Storage class in Azure  
--------------------------------------------------

# Defining a custome Storage class in Azure 

** kubectl apply -f CustomStorageClass.yaml

# get a list of current StoragClass

** kubectl get StoragClass

# Take a closer look at StoragClass, you can see the reclaim Policy is Delete since we didn't see it in our storage class

** kubectl describe StoragClass managed-standard-ssd

# Let's use our new storgeClass

** kubectl apply -f AzureDiskCustomStorageClass.yaml

# And take a closer look at our new Storage Class, Reclaim Policy Delete

** kubectl get PersistentVolumeClaim
** kubectl get PersistentVolume






























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































