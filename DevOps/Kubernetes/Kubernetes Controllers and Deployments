Managing Kubernetes Controllers and Deployments 
==========================================================

Kubernetes Principles:
---------------------------
1) Desired state and configuration (declrative approach) - We define our application deployment state in code and kubernetes will make that happen
2) Controllers - They keep our application or deployment in desired state. They monitor the current state of the cluster and if needed make changes to get the 
                 cluster in the desired state.
3) API server - Main information hub inside the kubernetes. Admins and other part of cluster such as controllers interact with K8 to understand the current state and 
                keep the things in sync with desired state.
				
Types of Controller Manager ( sometimes called controll loops )
----------------------------------------------------------------------

1) kube-controller manager - This is responsible for running core controller managers and keeping the cluster in the desired state using control loops which run 
                             on control plane node (Master node). Tehere is only 1 kube-controller manager per cluster 

2) cloud-controller manager - These are cloud specific controller-managers or control loops are originally implemented insdie the kube-controller manager but for spefic 
                              cloud to build controllers specific to the resources for the respective cloud.


Controller Operations
----------------------------

1) Watch Stste - It watches the state of the cluster through the API server and make changes in the current state to move the current state to the desired state.
2) Operations - Any operations that needed to move the cluster into the desired state will be generated by controller and submitted to API server.
3) API Server - it will handle the request and if needed will persist the change to etcd moving the cluster to current state to desired state


Controllers in Kubernetes 
--------------------------
1) Pod Controllers - a) ReplicaSet - 
                     b) Deployment - 
                     c) DaemonSet -	 
                     d) StatefulSet	-
                     e) Job -
                     f) CronJob - 					 

2) Other controllers - a) Node
                       b) Service
					   c) Endpoint
					   
Examining system Pods and thier supporting Controllers 
-----------------------------------------------------

# indside the kube-system namespace, there's a collection of controllers supporting the parts of the cluster's control plane node (Master Node)
# how would they get started since there is no cluster when they need to come online? static Pod Manifest

** kubectl get --namespaces kube-system all

# let's look more closely at one of those deployment, requiring 2 pods up a nd running at all the time ..

** kubectl get --namespace kube-system deployments coredns

# Daemonset Pods run on everynode in the cluster by default
#There's a Pod for our Pod network, Calico and one for the kube-proxy

** kubectl get --namespace kube-system daemonset

# We have 3 nodes, that is why for each daemonset they have 3 pods.

** kubectl get nodes 


Deployment Controllers 
------------------------------

1) It is to provide the declarative updates to replicaSets and thier consituents pods. we define and it will move us towards desire state
2) It provies the orchestration for things like pod creations and deletions.. it is driving us to to desired state which we have defined in deployment
3) we can use it to manatain the state of our applicatin of a time, rolloing out a new version or rolling back, we can scale up or scale out and these strategies we 
   can define in our deployment.
  It has 3 main types of Application state for Deplyment Controllers deployment
-----------------------------------------------------------------------------------

a) Creating Deployment - 1) declarative in nature
                         2) we write a deployment Spec in code (YAML)
						 3) there is a selector - it is used to identify the pods whihc are part of specfic deployment
						 4) Replicas - No of pods
                         5) Pod Template - It defines the technical details for the container based applications. These pods will be created by 
						    the replicaset for supporting the deployment

   
# Creating a Deployment Imperatively , with kubectl created and then scalling it 

** kubectl create deployment hello-word --image=nginx:latest
** kubectl scale deployment hello-word -- replicas=5
** kubectl get replicsets 
** kubectl get pods 
** kubectl describe pods |head -n 15


Understanding ReplicaSets (underlying building block of deployment)
----------------------------------------------------------------------

1) Deploys a defined number of Pods
2) Consists of Selector, Number of Replicas(pods) and a POd template 
3) Generally speaking we don't create Replicasets directly
4) we create deployment and they create replicasets for us 

Replicset Pod Operations
----------------------------

replicSets - very rarely we create it directly 

Just Example;
-------------

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: nginx
  namespace: playground1
  name: nginx
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.20.2
        name: nginx

1) ReplicSets allow for more complex, set based selectors
2) matchExpression as the selector
3) Operators - In, NotIn, Exists and DoesNotExist for looking key value selector 

Another Example
----------------------

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: nginx
  namespace: playground1
  name: nginx
spec:
  replicas: 5
  selector:
   matchExpressions:
    - key: app
	  operator : In
	  values:
		- hello-wordl-pod-metadata
   template:
    metadata:
      creationTimestamp: null
      labels:
        app: hello-wordl-pod-metadata


How ReplicaSets handles Failures:
--------------------------------------

1) Pod Failures (dies/ accidently deleted) - its responsibility of replicSet to reschedule and a new pod is started in the cluster.

2) Node Failure - a) Transient failure - unreachabilty for over a short period of time (reboot, n/w issues) : Pods on these nodes will mark error status on API
                                          server because controller manager can't reach the pods on that node . If node cmoes onli and pods are not running the 
										  container restart policy applies and kubelet restarts container inside the pods on that node. becasue the pods are 
										  still scheduled on that node.
										  
                  b) permanent failure - if node is gone permanently - on kube-controller-manager there is a pod-eviction-timeout (5 minutes default), 
				                         so post 5 minutes, the pods running on the node will be marked rerminated on API server and new pods will be created by
										 replicaset controller in cluster to ensure the application in desired state.

ReplicationControler is predesessor of ReplicSets and the difference is ReplicationControler has only single lable (key and value pair)
however ReplicSets allow for more expressive representations of state with set based selectors.

# demo - deploying a deployment

** kubectl apply -f deployment.yaml
** kubectl get replicset

# looking more closely at the replicaset selectors

** kubectl describe replicaset <replica_set_name>

# let's delete the deployment which will delete the replicaSets

** kubectl delete deployment <deployment_name>
** kubectl get replicset

# Deploy a ReplicaSet with matchExpression

** kubectl apply -f deployment <deployment_name>.yaml

# check the replicaset status

** kubectl get replicaset 

# looking more closely at the replicaset selectors again

** kubectl describe replicaset <replica_set_name>

# delete a Pod in a Replicaset, application will self heal itself 

** kubectl get pods
** kubectl delete pod <pod_name>
** kubectl get pods 

# Isolating a Pod from a replicSet 

** kubectl get pod --show-labels

# edit the label on one of the POds in replicaset, the replicaSet controller will create a new pod

** kubectl label pod <pod_name> app=Debug --overwrite
** kubectl get pods --show-labels

# taking over an existing Pod in a ReplicaSet, relable that pod to bring it back into the scope of the replicaset ... what K8 gonna do 

** kubectl label pod <pod_name> app=app_name --overwrite
** kubectl get pods --show-labels

NOTE - Kubernetes will delete one Pod and it will kepp the desired stste intact

** kubectl describe replicaset <replica_set_name>

# Demo - Node failure in Replicasets (shut down a node)

ssh to the worker node
** shutdown -h now

# Check the cluster status 

** kubectl get nodes 
** kubectl get nodes --watches

# But the pod is still on that node. Kubernetes is protecting against transient issues. Assumes the Pod is still running ...

** kubectl get pods -o wide

# Start up that node and break out watch when node reports Ready 

** kubectl get nodes --watches

# the pod on that node goes to error state then it will be restarted on that Node

** ** kubectl get pods -o wide

# it will start the container back up the on that node

# Demo - Node failure in Replicasets (shut down a node for more that 5 minutes)

ssh to the worker node
** shutdown -h now

# Check the cluster status and pod status 

** kubectl get nodes 
** kubectl get nodes --watches

# becuase of the -pod-eviction-timeout duration setting on kube-controller-manager, this pod will get kiiled

** kubectl get pods --watch

# Orphaned Pod goes terminating and a new pod will e deployed in the cluster.
# if the node retuurns the pod will be deleted, if node doen't then we have to delete it manually 

** kubectl get pods --watch 

**********************************************************************************************************************************************************************

b) Updating Deployments - Our dev team has created a new version of application and kept the images in the container registry and now its time to roll that out into 
                          K8 cluster. we can use a deployment to roll out new pod using new version of container image and the deployment will manage the transition  
                          between the two sets of the pods (pod set 1 - with older vresion and pod set 2- newer version). Once it is done we have all our pods running the 
						  new container image and the pods using the older iage will be terminated.
						
						  an Update in deployed in triggered by chnaging the Pod template(where we define pod and its configurtion). if we chnage any part in the pod 
						  template it will trigger a roll out as the new configurations will be pushed out to the cluster.
						  
                          few other fields can be chnaged without triggering an update for example no of replicsets


Controller Operations - Deployment Updates
-------------------------------------------------

1) Updating a Deployment Oject (imperatively)

** kubectl set image deployment <deployment_name> <deployment_name>=<container_image>
** kubectl set image deployment <deployment_name> <deployment_name>=<container_image> --record

--record --> It wll capture some additional metadata information about the chnage we just made and it will store this in Annotation associated with this deployment object
             
** kubectl edit deployment <deployment_name> -- this will get the deployment onject from API server and open loacally in a text editor on console. Once it gets opened up
                                                go the pod template make the changes save it. Then kubectl will ship the chnaged deployment back to API server to affect the
												chnage and initiate the deployment.
												
2) Updating a Deployment Oject (declaratively)												
								
** kubectl apply -f <deployment_name>.yaml --record 	



3) Checking the deployment status
--------------------------------------

** kubectl rollout status deployment <deployment_name> 
** kubectl describe deployment <deployment_name>

Deployment status 
-------------------

1) Complete - Its a stats where all the replicas are up to date and the previous replicas, pods are not running. New pods are running and ststus is ready.
2) progressing - when a new replicaSet has been created and is being scalled up (with new pod template) and the previous replicas are scalled down. 
3) Failed - the update cpuldn't be complete meaning the new replicaSet couldn't scaled to desired no.of replics or pods.


# Demo Updating a deployment (the image version is 1 here in this deployment)

** kubectl apply -f <deployment_name>.yaml

# to check the status of the deployment

** kubectl get deployment<deployment_name>   

# Now rolling out the new chnages (the image version is 2 here in this deployment)						

** kubectl apply -f <chnaged_deployment_name>.yaml

# Checking the ststus of that roll out, while command blocking your deployment is in progresing status

** kubectl rollout status deployment <chnaged_deployment_name>

# expect  return code 0 from kubectl rollout status

** echo $?

# let's walk through the description of deployment ...

** kubectl describe deployments <deployment_name> 

# Both replicset remain, and that will become very useful shortly when we use a rollback

** kubectl get replicset

# The new replicset checkout the labels, replicas, status and pod-template-hash

** kubectl describe replicset <new_replicaset_name>

# the old replicSet, checkout the labels, replicas, status and pod-template-hash

** kubectl describe replicset <old_replicaset_name>


Using Deployments to chnage State 
-----------------------------------

1) Kubernetes gives us the control to rollout of a new version of the application and this is called  
    a) Update Startgey 
    b) We can slso use k8 deployment to pasue roll out to make corrections
    c) Rollback to a previous version deployment
    d) restart of the Deployment 

Update Stratgey 
-------------------

1) RollingUpdate (default strategy) - A new replicaset starts scalling up and the oild replicSet will start scalling down to 0 and it manages the transition also.

2) Recreate - It will terminate al Pods in the current ReplicaSet set Prior to scalling up the new replicaSet. this strategy can be used when applications don'taking
              support running different versions concurrently

There are two configuration points:

This is all about the rate of scalling the pods in new replicaSet and the rate of scalling down pods in current/old replicaSet 

1) maxUnavailable - It ensures that a certain number of Pods are unavailable being updated at a partiular point of time. (by default the value is 25%)

2) maxSurge - It ensure that only a certain number of pods are created above the desired number of Pods.

Sucessfully Controlling a Deployment Rollouts 
-----------------------------------------------

1) Update Stratgey in Deployment spec
2) Readiness Probes in your Prod template spec

apiVersion: apps/v1
kind: Deployment
...
spec:
  replicas: 20
  strategy:
    type: RollingUpdate
     rollingUpdate: 
	   maxUnavailable : 20%
	   maxSurge: 5
		
   template:
...
     spec:
      container:
...
     readinessProbe:
	   httpGet: 
	     path: /index.html
         port: 8080
       initialdelaySeconds : 10  
       periodSeconds: 10


Pausing  Deployment 
----------------------

let's assume you forgot to add some configuration chnages in the deployment and the deployment has been initialised. If you edit the current deployment 
then apply it later it will have another deployment. So we want to stop the current rollout instaed edit option. 

So Kubernetes gives u sthe option to pause the rollout make the chnages and then resume the deployment.

1) Changes to the deployment while paused are not rolled out
2) Bacth chnages together, then resume the rollout
3) the current state of the Deployment is maintained until it's resumed
4) Starts up a new replicSet with the new chnages 

** kubectl rollout pause deployment <deployment_name>

** kubectl rollout resume deployment <deployment_name>

Rolling back Deployment 
----------------------------

in case of missconfiguration or any erorrs Kubernetes gives us option to roll back to a previous version of a Deployment

1) As we change onjects in Kubernetes, Deployment keep tracks of rollout history particularly it will track CHANGE-CAUSE as Annotation in Deployment
2) And withing that revision history of the chnages we made (replicaSets, pod template etc)
3) revisionhistoryLimit default value is 10
4) Number of replicaSets retained in History
5) This is very useful for roll back
6) revisionhistoryLimit can be set to ) for immediate cleanup 

** kubevctl rollout history deployment <deployment_name>

** kubevctl rollout history deployment <deployment_name> --revision=1 (the number is the specific version which we want to roll out)

** kubevctl rollout undo deployment <deployment_name>   this will simply roll out to the just previous version from the current deployment

** kubevctl rollout undo deployment <deployment_name> --to-revision=1 (this will simply roll out to the previous numbered version from the current deployment)

Restarting a deployment 
-----------------------------

1) effectively restarts all the pods 
2) But no Pod is ever "recreated"
3) new replicaSet will be created with the same Pod spec and scalling up new pods and scalling down of old pods will be handeled by kubernetes
4) uses Deployment's Update Strategy ( RollingUpdate and Recreate) 

** kubevctl rollout restart deployment <deployment_name>

# create a V1 deployment and then update it to v2 

** kubectl apply -f deployment.yaml
** kubectl apply -f deployment.v2.yaml

# Observe behaviour since new image wasn't availbale (we put wrong image name in deployment intentionaly) , the replicaSet doesn't go below maxUnavailable

** kubectl apply -f deployment.broken.yaml 

# why isn't that finishing ....?  After progressDeadlineSeconds which we ste to 10 Seconds (defaults to 10 minutes)

** kubectl rollout status deployment <deployment_name>

# let's examin ethe return code from the deployment.broken.yaml rollout

** echo $?

# lets check out Pods, ImagepullBack off/ErrImagpull .. ah! error in our image definition (we did it intentionaly)

kubectel get pods 

# What is maxUnavailable? 25% .. so only two pods in the ORIGINAL replicSets are offline and * are online 
# what is maxSurge? 25% So we have total 13 Pods. or 25% in addition to desired number 
# look at the replicas and Old replicaset 8/8 and NewReplicSet 5/5

** kubectl descriedeployments <deployment_name>

# Lets' sort this out now... check the rollout history, but which version should we rollback to ?

** kubectl rollout history deployment <deployment_name>

# It's easy in this example, but could be harder for complex system
# Let's look at our revision Annotation, should be 3

** kubectl describe deployments <deployment_name> | head 

# we can also look at the chnages applied in each revision to see the new pod templates.

** kubectl rollout history deployment <deployment_name> --revision=2
** kubectl rollout history deployment <deployment_name> --revision=3

# let's undo our rollout to revision 2, which is our V2 container

** kubectl rollout undo deployment <deployment_name> --to-revision=2
** kubectl rollout status deployment <deployment_name> 
** echo $?

# Checking the pods 

** kubectl get pods 

# cleanup time 

** kubectl delete deployment <deployment_name>
** kubectl delete service <service_name>

**********************************************************************************************************************************************************************

Rollout with UpdateStratgey and readiness Probes to controll a Rollout
----------------------------------------------------------------------------- 

# example of deployment.probes-1.yaml
 
 
apiVersion: apps/v1
kind: Deployment
...
spec:
  replicas: 20
  strategy:
    type: RollingUpdate
     rollingUpdate: 
	   maxUnavailable : 20%
	   maxSurge: 5
		
   template:
...
     spec:
      container:
...
     readinessProbe:
	   httpGet: 
	     path: /index.html
         port: 8080
       initialdelaySeconds : 10  
       periodSeconds: 10

# let's deploy a Deployment with readiness probes 

** kubectl apply -f deployment.probes-1.yaml --record

# also look there's a new annotation for our chnage-cause (Available pods still 0 becuase our Readiness probe's initialdelaySeconds is 10 seconds)

** kubectl describe deployment <deployment_name> 

# check again, replicas and condition, all Pods should be onLine and ready 

** kubectl describe deployment <deployment_name>

# Let's update the deploymnet V1 to V2 with Readiness Probes controlling the rollout, and record our rollout 

** diff deployment.probes-1.yaml deployment.probes-2.yaml
** kubectl apply -f deployment.probes-2.yaml --record

# Lot's of pods, most are not ready yet, but progressing ... how do we now it's progressing ?

** Kubectl get replicaset 
** kubectl describe deployment <deployment_name>

# let's update the deployment again to v3 

** kubectl apply -f deployment.probes-3.yaml --record

# lets's check out the rollout status 

** kubectl rollout status deployment <deployment_name>

# Let's check the status of the Deployment, Replicas and Conditions,
# 22 total (20 Original + 2 max surge)
# 18 available (20 original -2 (10%) in the old replicaSet)
# 4 Unavailable, (only 2 Pods in the old RS are Offline, 4 in the new replicaSet are not ready)

** kubectl describe deployment <deployment_name>

# let's look at the replicaset, no pods in the new replicSet are ready but 4 are deployed.

** kubectl get replicaset 

# ready .. that sounds familiar, let's check the deployment again
# What keeps a pod from reporting ready ?? A rediness probe ... see that Readiness probe, wrong port.

** kubectl describe deployment <deployment_name>

# we can read the deployment's rollout history, and see our chnage-cause annotations

** kubectl rollout history deployment <deployment_name>

# Let's rollback to revision 2 to undo that chnag..

** kubectl rollout history deployment <deployment_name> --revision=3
** kubectl rollout history deployment <deployment_name> --revision=2
** kubectl rollout undo deployment <deployment_name> --to-revision=2

# and check out the deployment to see if we get 20 ready replicas 

** kubectl describe deployment | head
** kubectl get deployment 

# cleanup time 

** kubectl delete deployment <deployment_name>
** kubectl delete service <service_name>


Demo Restarting a deployment 
-----------------------------

# creating a deployment 

** kubectl create deployment <deployment_name> --image=nginx --replicas=5

# checking the ststus of the deployment 

** kubectl get deployment 
** kubectl get pods 

# Let's restart a deployment 

** kubectl rollout restart deployment <deployment_name>


# WE gonna get a new ReplicSet and Pods in the old replicaset are shutting down 

** kubectl get deployment 
** kubectl get pods

*************************************************************************************************************************************************************************

c) Scaling Deployment  - Kubernetes gives us 2 ways 

a) Manually - ** kubectl scale deployment <deployment_name> -replicas=10 
              ** kubectl apply -f deployment.yaml 


b) Horizontal Pod Autoscaler - It is s Kubernetes resource that scales no of pods in a replicaset/deploymnet based on resource utilization and chnages inside 
                               this are driven by controller manager 


# lets start off imperatively creating deployment and sacliing it.

** kubectl create deployment <deployment_name> --image=nginx

# checking the ststus of the deployment 

** kubectl get deployment 
** kubectl get pods 

# lets scale the deployment from 1 to 10 replicas

** kubectl scale deployment <deployment_name> -replicas=10

# Let's check out the deployment to see if we get 10 ready replicas 

** kubectl get deployment 
** kubectl describe deployment | head

# lets ssee the declaratively way of creating deployment and sacliing it.

** kubectl apply -f deployment <deployment_name>

# chnaging the replicaset value =20 in new deployment.20replicas.yaml file and initiating it 

** diff deployment.yaml deployment.20replicas.yaml
** kubectl apply -f deploymentdeployment.20replicas.yaml

# Let's check out the deployment to see if we get 20 ready replicas 

** kubectl get deployment <deployment_name>
** kubectl describe deployment 

**********************************************************************************************************************************************************************

Now we gonna work with Some additional Controllers in Kubernetes :
-----------------------------------------------------------------------

1) DaemonSet  - A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are 
                removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

1) These are useful when we need to run a single pod on every node or a subets of nodes in the cluster. 
2) Ensures that all or some Nodes run a Pod
3) Effectiviley an init daemon inside your cluster
4) Example - a) kube-proxy
			 b) log collectors
             c) metric servers
			 d) Resources monitoring agents 
			 e) Storage daemons

DaemonSet Update Strategy
-------------------------------

1) RollingUpdate - After you update the daemonset pod template old pods will be kiiled and new pods will be created in a controlled way. 
                   defualt update strategy for daemonset

2) OnDelete - After you update the daemonset pod template new pods will only be created when we go and delete manually the existing pods in the cluster


NOTE- there is no pauing and resuming rollout concept here. So if anything goes wrong define/correct the things in object again and initialize it.

DEMO - Creation of DaemonSet on all Nodes
---------------------------------------------

# We get one Pod per node to run the network services on that Node

** kubetctl get pods
** kubectl get daemonsets --namespace kube-system kube-proxy

# Let's create a DaemonSet with Pods on each Node in our Cluster .. that's Not the master

** kubectl appy -f DaemonSet.yaml 

# So we get three since we have 3 workers and 1 master in our cluster and master is only ste to run system pods

** kubectl get daemonsets
** kubectl get daemonsets -o wide
** kubectl get pods

# CallOut, labels, Desired?current Nodes Scheduled. Pods status and template and Events.

** kubectl get daemonsets <daemonset_name> | more

# To check the pod labels, controler-revision-hash and pod template-generation

** kubectl get pods --show-labels

# if we chnage the lable to one of ur pods ..

** kubectl label pod <pod_name> app=not-hello-worl --overwrite

# we gonna get a new pod from DaemonSet Controller

** kubectl get pods --show-labels 


DEMO - Creation of DaemonSet on a SubSet Nodes
-------------------------------------------------

# lets create a DaemonSetwith a defined NodeSelector

** kubectl appy -f DaemonSetwithNodeSelector .yaml

# No pods got created becuas we don't have anynodes with Appropriate label

** kubectl get daemonsets

# we need a Node that satisfies the Node Selector 

** kubectl label node <node_name> node=hello-world-ns

# let's see if a Pods gets created

** kubectl get daemonsets
** kubectl get daemonsets -o wide
** kubectl get pods -o wide

# what if we remove the lable 

** kubectl label node <node_name> node-

# It's going to terminate the pod, examine the evenets, Desired Number of Nodes Scheduled...

** kubectl describe daemonsets <daemonset_name>


DEMO - Update of DaemonSet 
----------------------------

# Let's create a DaemonSet again 

** kubectl appy -f DaemonSet.yaml 

# Check out the image version, 1.0

** kubectl describe daemonsets <daemonset_name>

# Examine what our update strategy is .default to rollingUpdate and maxUnavailable 1

** kubectl get daemonsets <daemonset_name> -o yaml | more

# Update our container image from 1.0 to 2.0 and apply the config

** diff DaemonSet.yaml DaemonSet-v2.yaml
** kubectl appy -f DaemonSet-v2.yaml 

# Checkout the rolout status, a much slower that a deployment due to max unavailable

** kubectl rollout status daemonsets <daemonset_name>

# we can see our daemonset container image is 2.0 now and in the events that is rolled out.

** ** kubectl describe daemonsets

# we can see a new controler-revision-hash and also an updated pod template-generation

** kubectl get pods --show-labels

************************************************************************************************************************************************************************

Controllers so far introduced, start up and run Pods that continuously... But if we want to run a single task periodically in cluster

2) Jobs and CronJobs - Job is a back job or single task and it gurantees to complete successful and it can be scheduled also (Cron Job)

a) Jobs create one or more Pods
b) Runs a program in a container to completion
c) Ensure that specified number of POds complete successfully
d) Workload examples - Ad-hoc Jobs, batch jobs, data oriented Job 

If anything goes wrong it is the responsibility of job controller to complete the job successfully. 2 methods is used to accomplish it,.

1) Interrupted Execution - if a pod execution is interrupted (due a node failure for example) the Job controler will asign the pod to other node in cluster.
  
2) Non-Zero Exit Code - if a container based application running in a pod controlled by a job controller return a non-zero exit code then its responsibility 
                        of job controller will consider restart policy of the container (default restartPolicy is not compatible job controller) and act accordingly. 
						so either it should be onFailure (the job contraoller wil try again ) or never  (job controller will mark the job as failed)


Jobs Lifecycle
====================

1) Jobs are task that we need to ensure run to completion
2) When a job completes and exit code is 0 then its status is set to be completed 
3) the Job object remains and the pods are deleted 
4) the reason - this way we can keep them around for thier logs and other output 
5) however it is up to us if we want to delete the job when its finished, this will delete the Pods.

Example
----------------

apiVersion: apps/v1
kind: Job
metadata:
	name: hello-world
spec:
  template:
    spec:
      containers:
      - image: ubuntu
	    name: ubuntu
		command:
			- "/bin/bash"
			- "-c"
			- "/bin/echo hellofrom POd $(hostname) at $(date)
		restartPolicy: never


Controlling Job execution
==================================

1) backoffLimit - number of Job retries before its' marked failed. (default value is 6)

2) activeDeadlineSeconds - max execution time for the job

3) parallelism - max number of running Pods in a Job at a point in time

4) completions - number of Pods that need to finish successfully 


Introducing Cron Jobs
============================

1) CronJobs will run a job on a given time based scope
2) Comceptually similar to UNIX/LINUX cron job
3) Usees the standard cron format
4) example Workloads - periodic and scheduled tasks
5) CronJobs resource is created when the object is submitted to the API server 
6) When it's time, a Job is created via the Job template from the CronJob object 

Controlling CronJob execution
==================================

1) Schedule - a cron formatted schedule

2) Suspend - suspends the CronJob

3) startingDeadlinseconds - the job hasn't started in this amount of time then mark it as failed 

4) concurrencyPolicy - handels concurrent executions of a Job, Allow, forbid or Replace ( assume a job is still running and the next execution schedule has arrived) 


Example
----------------

apiVersion: apps/v1
kind: CronJob
metadata:
	name: hello-world-cron
spec:
  schedule: '*/1 * * * *'
  jobTemplate: 
    spec:
      template:
	     containers:
			- image: ubuntu
			  name: ubuntu
		      command:
			  - "/bin/bash"
			  - "-c"
			  - "/bin/echo hellofrom POd $(hostname) at $(date)
		  restartPolicy: never

DEMO - Executing with jobs  
----------------------------

# we will need Onfailure or never, Let's look at the Onfailure

** kubectl apply -f job.yaml

# Follow the Job Status with watch 

** kubectl get job --watch 

# get a lists of Pod, Status is completed and ready is 0/1

** kubectl get pods 

#  let's get some more details about the job... lables and selectors, start time, Duration and Pod statuses

** kubectl describe job <job_name>

# get the logs from stdout from the Job Pod

** kubectl get pods -l job-name=hello-world-job
** kubectl logs <pod_name>



DEMO - job failure and restart Failure  
-------------------------------------------

# we will want to use never, so our pods are never deleted after backofflimit is reached 

** kubectl apply -f job-failure-Onfailure.yaml

# let's look at the pods, enters a backoffloop after 2 crashes

** kubectl get pods --watch 

# the pods aren't deleted so we can troubleshoot here if needed.

** kubectl get pods 

# And the job won't have any completions and it doesn't get deleted

** kubectl get jobs

# So let's review what the job did... events , created and then deleted, Pod status , 3 failed 

** kubectl describe jobs | more

# Cleanup job time

** kubectl delete jobs <job_name>
** kubectl get pods 


DEMO - Working with parallel jobs and scheduling tasks with cronJobs  
-------------------------------------------------------------------------

# Defining a parallel Job

** kubectl apply -f paralleljob.yaml

# 10 Pods will run in parallel up until 50 completions

** kubectl get pods

# we can "watch" the status with watch

** watch 'kubectl describe job | head -n 11 '
** crtl + c 
** kubectl get pods 

# Defining a cronJob Job

** kubectl apply -f cronjob.yaml

# qucik overview of the job and its schedule

** kubectl get cronJobs

# But let's take a closer look... schedule, concurrency, suspend, start deadline seconds, events .. execution history 

** kubectl describe cronjobs | more

# get an overview again 

** ubectl get cronJobs

# the pods will stick around , in event we need thier logs or other informstion. how long ?

** kubectl get pods --watch 

# they will stick around for successfulJobsHistorylimit, which defaults to three

** kubectl get cronjobs -o yaml 

# Cleanup cronjob time

** kubectl delete cronjobs <job_name>
** kubectl get pods 


*********************************************************************************************************************************************************************

3) StatefulSets - This is the controller type which provides needed infra for stateful applications (ex - persisted namings and orderd operations and persting storage)

a) they enables stateful applications to be managed by a controler 
example 

a) DB worklaod
b) caching servers
c) Application state for web farms 


StatefulSets Capabilities
----------------------------

whenever a pos is rescheduled its a completly new pod evertime for stateful application we should have ability when the pods gets 
rescheduled it shul maintain its state 

It is done by the below 3 

1) persistent order naming  -  unique names 

2) Persistent staorage - to stored the data in named loaction so they can come back 

3) headless service - these are the services that don't have loadbalancers or cluster Ips and give ability to our stateful application to loacate eac other 
                       using clusterdns 
























 

							






































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































